{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b7353839",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scanpy as sc\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv\n",
    "from torch_geometric.data import Data\n",
    "import torch.optim as optim\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf1f9407",
   "metadata": {},
   "outputs": [],
   "source": [
    "#file = r\"C:\\Users\\ouyan\\Downloads\\BrainAgingSpatialAtlas_Imputed.h5ad\"\n",
    "#adata_full = sc.read_h5ad(file, backed='r')\n",
    "\n",
    "#print(adata_full)\n",
    "#print(\"obs columns:\", adata_full.obs.columns)\n",
    "#print(\"var columns:\", adata_full.var.columns)\n",
    "#print(\"obsm keys:\", adata_full.obsm_keys())\n",
    "#print(\"layers:\", adata_full.layers.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e972b86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(\"Available slice IDs:\", adata_full.obs['slice'].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6eb1666",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Êà™ÂèñÂ≠êÈõÜÂπ∂Â§çÂà∂Âà∞ÂÜÖÂ≠ò\n",
    "#TEST_SLICE_ID = '1'\n",
    "#print(f\"2. Creating subset for slice ID '{TEST_SLICE_ID}'...\")\n",
    "#adata = adata_full[adata_full.obs['slice'] == TEST_SLICE_ID, :].to_memory() \n",
    "\n",
    "# ÈáäÊîæ full data ÁöÑÂºïÁî®\n",
    "#del adata_full\n",
    "#print(f\"‚úÖ Subset created. New shape: {adata.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ebb2025",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Loading data in backed mode to conserve memory...\n",
      "2. Creating subset for slice ID '1' and forcing memory conversion...\n",
      "‚úÖ Subset created. New shape: (144249, 7276)\n"
     ]
    }
   ],
   "source": [
    "# --- A. Êï∞ÊçÆÂä†ËΩΩ‰∏éÂ≠êÈõÜÂàõÂª∫ (ÊúÄÁªà‰øÆÊ≠£Áâà) ---\n",
    "\n",
    "file = r\"C:\\Users\\ouyan\\Downloads\\BrainAgingSpatialAtlas_Imputed.h5ad\"\n",
    "TEST_SLICE_ID = '1'\n",
    "\n",
    "print(f\"1. Loading data in backed mode to conserve memory...\")\n",
    "adata_full = sc.read_h5ad(file, backed='r') \n",
    "\n",
    "# 2. Êà™ÂèñÂ≠êÈõÜÂπ∂ÊòæÂºèÂä†ËΩΩÂà∞ÂÜÖÂ≠ò\n",
    "print(f\"2. Creating subset for slice ID '{TEST_SLICE_ID}' and forcing memory conversion...\")\n",
    "\n",
    "# Ê≠•È™§ 2a: ÂàáÁâá\n",
    "adata_subset_view = adata_full[adata_full.obs['slice'] == TEST_SLICE_ID, :]\n",
    "\n",
    "# Ê≠•È™§ 2b: Â∞ÜÂàáÁâáËßÜÂõæËΩ¨Êç¢‰∏∫‰∏Ä‰∏™ÂÖ®Êñ∞ÁöÑ AnnData ÂØπË±° (ÈÄöÂ∏∏ËøôÂ∞±‰ºöËß£ÂÜ≥ÈóÆÈ¢ò)\n",
    "adata = adata_subset_view.to_memory()\n",
    "\n",
    "# Ê≠•È™§ 2c: üö® Ê†∏ÂøÉ‰øÆÊ≠£ÔºöÁ°Æ‰øù X ÊòØÊ†áÂáÜÁöÑ scipy sparse matrix (CSR)\n",
    "# Ëøô‰∏ÄÊ≠•Â∞Ü _CSRDataset ÊòéÁ°ÆËΩ¨Êç¢‰∏∫Ê†áÂáÜÁöÑ CSRMatrixÔºå‰ª•ÈÄöËøá scanpy ÁöÑÁ±ªÂûãÊ£ÄÊü•\n",
    "if not isinstance(adata.X, sp.csr_matrix):\n",
    "    adata.X = adata.X.tocsr()\n",
    "    print(\"   -> adata.X successfully converted to scipy.sparse.csr_matrix.\")\n",
    "\n",
    "\n",
    "# ÈáäÊîæ full data ÁöÑÂºïÁî®\n",
    "del adata_full\n",
    "print(f\"‚úÖ Subset created. New shape: {adata.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b1eaf83",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> ‰ºòÂåñÁ®ÄÁñèÁü©ÈòµÁ¥¢ÂºïÂíåÊåáÈíàÁ±ªÂûã...\n",
      "‚úÖ ÂÜÖÂ≠ò‰ºòÂåñÂÆåÊàê„ÄÇX dtype: float32, Indices dtype: int32\n"
     ]
    }
   ],
   "source": [
    "# 1. Âº∫Âà∂Â∞ÜÊï∞ÊçÆÁü©Èòµ X ÁöÑÊï∞ÊçÆÁ±ªÂûã‰ªé float64 ËΩ¨Êç¢‰∏∫ float32„ÄÇ\n",
    "# ËøôÈÄöÂ∏∏ÂèØ‰ª•Â∞ÜÂÜÖÂ≠òÈúÄÊ±ÇÂáèÂ∞ë‰∏ÄÂçä„ÄÇ\n",
    "if adata.X.dtype != np.float32:\n",
    "    adata.X = adata.X.astype(np.float32)\n",
    "    print(f\"-> adata.X Â∑≤ËΩ¨Êç¢‰∏∫ {adata.X.dtype}\")\n",
    "\n",
    "# 2. ‰ºòÂåñÁ®ÄÁñèÁü©ÈòµÁöÑÁ¥¢ÂºïÁ±ªÂûã (Indices/Indptr)\n",
    "# ËôΩÁÑ∂ÈîôËØØÊòæÁ§∫ int32Ôºå‰ΩÜÂ¶ÇÊûúÂÆÉ‰ª¨ÊòØ int64ÔºåÂº∫Âà∂ËΩ¨Êç¢‰πüËÉΩËäÇÁúÅÂÜÖÂ≠ò„ÄÇ\n",
    "if sp.issparse(adata.X):\n",
    "    print(\"-> ‰ºòÂåñÁ®ÄÁñèÁü©ÈòµÁ¥¢ÂºïÂíåÊåáÈíàÁ±ªÂûã...\")\n",
    "    # Á°Æ‰øùÊòØ CSR Ê†ºÂºè\n",
    "    adata.X = adata.X.tocsr(copy=False) \n",
    "    \n",
    "    # Âº∫Âà∂Â∞ÜÁ¥¢ÂºïÂíåÊåáÈíàËΩ¨Êç¢‰∏∫ int32Ôºå‰ª•‰øùËØÅÊúÄÂ∞èÂÜÖÂ≠òÂç†Áî®\n",
    "    adata.X.indices = adata.X.indices.astype(np.int32)\n",
    "    adata.X.indptr = adata.X.indptr.astype(np.int32)\n",
    "    \n",
    "    # Ê∏ÖÁêÜÁ®ÄÁñèÁü©ÈòµÁªìÊûÑÔºàÂèØÈÄâÔºå‰ΩÜÊé®ËçêÔºâ\n",
    "    adata.X.eliminate_zeros()\n",
    "    adata.X.check_format()\n",
    "\n",
    "print(f\"‚úÖ ÂÜÖÂ≠ò‰ºòÂåñÂÆåÊàê„ÄÇX dtype: {adata.X.dtype}, Indices dtype: {adata.X.indices.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a77f5e6",
   "metadata": {},
   "source": [
    "ÂØπCellÂÅönormalizationÂíålog transform, ‰ª•ÂèäÂØπÊØè‰∏™ gene ÂÅöÊ†áÂáÜÂåñscaleÔºåËÆ©ÊØè‰∏™Âü∫Âõ†Âú® PCA Êó∂Ë¥°ÁåÆÂ§ßËá¥Áõ∏Á≠â"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02bd9bcf",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.91 GiB for an array with shape (1049555723,) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mMemoryError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 1. Âü∫Êú¨ËøáÊª§\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43msc\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfilter_cells\u001b[49m\u001b[43m(\u001b[49m\u001b[43madata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmin_genes\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      3\u001b[39m sc.pp.filter_genes(adata, min_cells=\u001b[32m5\u001b[39m)\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# 2. ÊÄªËÆ°Êï∞ÂΩí‰∏ÄÂåñ\u001b[39;00m\n",
      "    \u001b[31m[... skipping hidden 1 frame]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ouyan\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\scanpy\\preprocessing\\_simple.py:177\u001b[39m, in \u001b[36mfilter_cells\u001b[39m\u001b[34m(data, min_counts, min_genes, max_counts, max_genes, inplace, copy)\u001b[39m\n\u001b[32m    175\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    176\u001b[39m         adata.obs[\u001b[33m\"\u001b[39m\u001b[33mn_genes\u001b[39m\u001b[33m\"\u001b[39m] = number\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[43madata\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_inplace_subset_obs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcell_subset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m adata \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    180\u001b[39m min_number = min_counts \u001b[38;5;28;01mif\u001b[39;00m min_genes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m min_genes\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ouyan\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\anndata\\_core\\anndata.py:1197\u001b[39m, in \u001b[36mAnnData._inplace_subset_obs\u001b[39m\u001b[34m(self, index)\u001b[39m\n\u001b[32m   1191\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_inplace_subset_obs\u001b[39m(\u001b[38;5;28mself\u001b[39m, index: Index1D):\n\u001b[32m   1192\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\\\u001b[39;00m\n\u001b[32m   1193\u001b[39m \u001b[33;03m    Inplace subsetting along variables dimension.\u001b[39;00m\n\u001b[32m   1194\u001b[39m \n\u001b[32m   1195\u001b[39m \u001b[33;03m    Same as `adata = adata[index, :]`, but inplace.\u001b[39;00m\n\u001b[32m   1196\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1197\u001b[39m     adata_subset = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1199\u001b[39m     \u001b[38;5;28mself\u001b[39m._init_as_actual(adata_subset)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ouyan\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\anndata\\_core\\anndata.py:1474\u001b[39m, in \u001b[36mAnnData.copy\u001b[39m\u001b[34m(self, filename)\u001b[39m\n\u001b[32m   1467\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.isbacked:\n\u001b[32m   1468\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.is_view \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._has_X():\n\u001b[32m   1469\u001b[39m         \u001b[38;5;66;03m# TODO: How do I unambiguously check if this is a copy?\u001b[39;00m\n\u001b[32m   1470\u001b[39m         \u001b[38;5;66;03m# Subsetting this way means we don‚Äôt have to have a view type\u001b[39;00m\n\u001b[32m   1471\u001b[39m         \u001b[38;5;66;03m# defined for the matrix, which is needed for some of the\u001b[39;00m\n\u001b[32m   1472\u001b[39m         \u001b[38;5;66;03m# current distributed backend. Specifically Dask.\u001b[39;00m\n\u001b[32m   1473\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mutated_copy(\n\u001b[32m-> \u001b[39m\u001b[32m1474\u001b[39m             X=\u001b[43m_subset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_adata_ref\u001b[49m\u001b[43m.\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_oidx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_vidx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.copy()\n\u001b[32m   1475\u001b[39m         )\n\u001b[32m   1476\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1477\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._mutated_copy()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ouyan\\anaconda3\\envs\\torch_env\\Lib\\functools.py:909\u001b[39m, in \u001b[36msingledispatch.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kw)\u001b[39m\n\u001b[32m    905\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[32m    906\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfuncname\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires at least \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    907\u001b[39m                     \u001b[33m'\u001b[39m\u001b[33m1 positional argument\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m909\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdispatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ouyan\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\anndata\\_core\\index.py:200\u001b[39m, in \u001b[36m_subset_sparse\u001b[39m\u001b[34m(a, subset_idx)\u001b[39m\n\u001b[32m    198\u001b[39m         first_idx = np.flatnonzero(first_idx)\n\u001b[32m    199\u001b[39m     subset_idx = (first_idx.reshape(-\u001b[32m1\u001b[39m, \u001b[32m1\u001b[39m), *subset_idx[\u001b[32m1\u001b[39m:])\n\u001b[32m--> \u001b[39m\u001b[32m200\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[43msubset_idx\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ouyan\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\scipy\\sparse\\_index.py:87\u001b[39m, in \u001b[36mIndexMixin.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m     85\u001b[39m     res = \u001b[38;5;28mself\u001b[39m._get_arrayXint(row, col)\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(col, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m87\u001b[39m     res = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_arrayXslice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# arrayXarray preprocess\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m (row.ndim == \u001b[32m2\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m row.shape[\u001b[32m1\u001b[39m] == \u001b[32m1\u001b[39m\n\u001b[32m     90\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m (col.ndim == \u001b[32m1\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m col.shape[\u001b[32m0\u001b[39m] == \u001b[32m1\u001b[39m)):\n\u001b[32m     91\u001b[39m     \u001b[38;5;66;03m# outer indexing\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ouyan\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\scipy\\sparse\\_csr.py:277\u001b[39m, in \u001b[36m_csr_base._get_arrayXslice\u001b[39m\u001b[34m(self, row, col)\u001b[39m\n\u001b[32m    275\u001b[39m     col = np.arange(*col.indices(\u001b[38;5;28mself\u001b[39m.shape[\u001b[32m1\u001b[39m]))\n\u001b[32m    276\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._get_arrayXarray(row, col)\n\u001b[32m--> \u001b[39m\u001b[32m277\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_major_index_fancy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m)\u001b[49m._get_submatrix(minor=col)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\ouyan\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:593\u001b[39m, in \u001b[36m_cs_matrix._major_index_fancy\u001b[39m\u001b[34m(self, idx)\u001b[39m\n\u001b[32m    590\u001b[39m np.cumsum(row_nnz, out=res_indptr[\u001b[32m1\u001b[39m:])\n\u001b[32m    592\u001b[39m nnz = res_indptr[-\u001b[32m1\u001b[39m]\n\u001b[32m--> \u001b[39m\u001b[32m593\u001b[39m res_indices = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mempty\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnnz\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43midx_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    594\u001b[39m res_data = np.empty(nnz, dtype=\u001b[38;5;28mself\u001b[39m.dtype)\n\u001b[32m    595\u001b[39m csr_row_index(\n\u001b[32m    596\u001b[39m     M,\n\u001b[32m    597\u001b[39m     indices,\n\u001b[32m   (...)\u001b[39m\u001b[32m    602\u001b[39m     res_data\n\u001b[32m    603\u001b[39m )\n",
      "\u001b[31mMemoryError\u001b[39m: Unable to allocate 3.91 GiB for an array with shape (1049555723,) and data type int32"
     ]
    }
   ],
   "source": [
    "# 1. Âü∫Êú¨ËøáÊª§\n",
    "#sc.pp.filter_cells(adata, min_genes=5)\n",
    "#sc.pp.filter_genes(adata, min_cells=5)\n",
    "\n",
    "# 2. ÊÄªËÆ°Êï∞ÂΩí‰∏ÄÂåñ\n",
    "#sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "\n",
    "# 3. log1p ÂèòÊç¢\n",
    "#sc.pp.log1p(adata)\n",
    "\n",
    "# 4. ÂèØÈÄâÔºöÈ´òÂèòÂü∫Âõ†Á≠õÈÄâ\n",
    "#sc.pp.highly_variable_genes(adata, n_top_genes=2000)\n",
    "#adata = adata[:, adata.var.highly_variable]\n",
    "#print(f\"Shape after HVG: {adata.shape}\")\n",
    "\n",
    "# 5. scale\n",
    "#sc.pp.scale(adata, max_value=10)\n",
    "\n",
    "# 6. PCA\n",
    "#sc.tl.pca(adata, n_comps=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "90ac8c47",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. ÊâãÂä®ËÆ°ÁÆóÊØè‰∏™ÁªÜËÉûÁöÑÈùûÈõ∂Âü∫Âõ†Êï∞ (min_genes=5)...\n",
      "   -> ÂèëÁé∞ 144249 ‰∏™ÁªÜËÉû (100.00%) Êª°Ë∂≥Ë¶ÅÊ±Ç„ÄÇ\n",
      "2. ÊâãÂä®ËÆ°ÁÆóÊØè‰∏™Âü∫Âõ†ÁöÑË°®ËææÁªÜËÉûÊï∞ (min_cells=5)...\n",
      "   -> ÂèëÁé∞ 7271 ‰∏™Âü∫Âõ† (99.93%) Êª°Ë∂≥Ë¶ÅÊ±Ç„ÄÇ\n",
      "\n",
      "3. Â∫îÁî®ÁªÜËÉûÂíåÂü∫Âõ†Â≠êÈõÜ...\n",
      "‚úÖ ËøáÊª§ÊàêÂäü„ÄÇÊñ∞ÂΩ¢Áä∂: (144249, 7271)\n",
      "\n",
      "4. ÁªßÁª≠ËøêË°åÂΩí‰∏ÄÂåñ...\n",
      "‚úÖ ÂΩí‰∏ÄÂåñÂÆåÊàê„ÄÇ\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ouyan\\anaconda3\\envs\\torch_env\\Lib\\site-packages\\legacy_api_wrap\\__init__.py:88: UserWarning: Some cells have zero counts\n",
      "  return fn(*args_all, **kw)\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------\n",
    "# Ê≠•È™§ 1: ÊâãÂä®ËøáÊª§ÁªÜËÉû (sc.pp.filter_cells Êõø‰ª£)\n",
    "# ---------------------------------------------\n",
    "\n",
    "min_genes_threshold = 5 \n",
    "print(f\"1. ÊâãÂä®ËÆ°ÁÆóÊØè‰∏™ÁªÜËÉûÁöÑÈùûÈõ∂Âü∫Âõ†Êï∞ (min_genes={min_genes_threshold})...\")\n",
    "\n",
    "# ËÆ°ÁÆóÊØè‰∏™ÁªÜËÉûÊúâÂ§öÂ∞ë‰∏™Âü∫Âõ†Ë°®Ëææ (ËÆ°Êï∞ > 0)\n",
    "# .A1 Áî®‰∫éÂ∞Ü Scipy Á®ÄÁñèÁü©ÈòµÁöÑÁªìÊûúÔºàÈÄöÂ∏∏ÊòØ 1xN Êàñ Nx1 Áü©ÈòµÔºâËΩ¨Êç¢‰∏∫ 1D numpy Êï∞ÁªÑ\n",
    "n_genes_by_cell = np.ravel((adata.X > 0).sum(axis=1)) \n",
    "\n",
    "# ÂàõÂª∫‰∏Ä‰∏™Â∏ÉÂ∞îÊé©Á†ÅÔºöÊâæÂà∞Ë¶Å‰øùÁïôÁöÑÁªÜËÉû\n",
    "cell_subset = n_genes_by_cell >= min_genes_threshold\n",
    "cells_to_keep = np.sum(cell_subset)\n",
    "\n",
    "# Â∞ÜÁªìÊûúÂ≠òÂÇ®Âú® adata.obs ‰∏≠\n",
    "adata.obs['n_genes'] = n_genes_by_cell\n",
    "print(f\"   -> ÂèëÁé∞ {cells_to_keep} ‰∏™ÁªÜËÉû ({cells_to_keep / adata.shape[0]:.2%}) Êª°Ë∂≥Ë¶ÅÊ±Ç„ÄÇ\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Ê≠•È™§ 2: ÊâãÂä®ËøáÊª§Âü∫Âõ† (sc.pp.filter_genes Êõø‰ª£)\n",
    "# ---------------------------------------------\n",
    "\n",
    "min_cells_threshold = 5\n",
    "print(f\"2. ÊâãÂä®ËÆ°ÁÆóÊØè‰∏™Âü∫Âõ†ÁöÑË°®ËææÁªÜËÉûÊï∞ (min_cells={min_cells_threshold})...\")\n",
    "\n",
    "# ËÆ°ÁÆóÊØè‰∏™Âü∫Âõ†ÊúâÂ§öÂ∞ë‰∏™ÁªÜËÉûË°®Ëææ (ËÆ°Êï∞ > 0)\n",
    "n_cells_by_gene = np.ravel((adata.X > 0).sum(axis=0)) \n",
    "\n",
    "# ÂàõÂª∫‰∏Ä‰∏™Â∏ÉÂ∞îÊé©Á†ÅÔºöÊâæÂà∞Ë¶Å‰øùÁïôÁöÑÂü∫Âõ†\n",
    "gene_subset = n_cells_by_gene >= min_cells_threshold\n",
    "genes_to_keep = np.sum(gene_subset)\n",
    "\n",
    "# Â∞ÜÁªìÊûúÂ≠òÂÇ®Âú® adata.var ‰∏≠\n",
    "adata.var['n_cells'] = n_cells_by_gene\n",
    "print(f\"   -> ÂèëÁé∞ {genes_to_keep} ‰∏™Âü∫Âõ† ({genes_to_keep / adata.shape[1]:.2%}) Êª°Ë∂≥Ë¶ÅÊ±Ç„ÄÇ\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Ê≠•È™§ 3: ‰ΩøÁî®ÂàáÁâáÂ∫îÁî®Â≠êÈõÜ\n",
    "# ---------------------------------------------\n",
    "\n",
    "print(\"\\n3. Â∫îÁî®ÁªÜËÉûÂíåÂü∫Âõ†Â≠êÈõÜ...\")\n",
    "\n",
    "# ‰ΩøÁî®ÂàáÁâáÊìç‰ΩúÂ∫îÁî®ËøáÊª§ÁªìÊûú„ÄÇ\n",
    "# AnnData ÁöÑÂàáÁâáÊìç‰Ωú (adata[index, :]) Â∫îËØ•ÊØîÂÜÖÈÉ®ÁöÑ .copy() Êõ¥È´òÊïà„ÄÇ\n",
    "# Ê≥®ÊÑèÔºöÊàë‰ª¨‰øùÁïô .copy() ‰ª•Á°Æ‰øùÂæóÂà∞‰∏Ä‰∏™Êñ∞ÁöÑ„ÄÅÂÜÖÂ≠òËøûÁª≠ÁöÑÂØπË±°ÔºåËÄå‰∏çÊòØ‰∏Ä‰∏™ view„ÄÇ\n",
    "# Â¶ÇÊûúËøô‰∏ÄË°åÂÜçÊ¨°Â§±Ë¥•ÔºåËØ∑Âà†Èô§ .copy()\n",
    "try:\n",
    "    adata = adata[cell_subset, gene_subset].copy()\n",
    "    print(f\"‚úÖ ËøáÊª§ÊàêÂäü„ÄÇÊñ∞ÂΩ¢Áä∂: {adata.shape}\")\n",
    "except MemoryError:\n",
    "    # ÊúÄÂêéÁöÑÂ∞ùËØïÔºöÂàõÂª∫‰∏Ä‰∏™ view\n",
    "    print(\"‚ö†Ô∏è .copy() ÂÜçÊ¨°Â§±Ë¥•„ÄÇÂ∞ùËØïÂàõÂª∫ view (adata = adata[...])„ÄÇ\")\n",
    "    adata = adata[cell_subset, gene_subset]\n",
    "    print(f\"‚úÖ ËøáÊª§ÊàêÂäü (View)„ÄÇÊñ∞ÂΩ¢Áä∂: {adata.shape}\")\n",
    "\n",
    "\n",
    "# ---------------------------------------------\n",
    "# Ê≠•È™§ 4: ÁªßÁª≠È¢ÑÂ§ÑÁêÜ (‰æãÂ¶ÇÂΩí‰∏ÄÂåñ)\n",
    "# ---------------------------------------------\n",
    "\n",
    "print(\"\\n4. ÁªßÁª≠ËøêË°åÂΩí‰∏ÄÂåñ...\")\n",
    "# Â¶ÇÊûú adata ÊòØ‰∏Ä‰∏™ view (‰∏ä‰∏ÄÊ≠•Âà†Èô§ .copy() ÁöÑÊÉÖÂÜµ)ÔºåÂÆÉÂèØËÉΩ‰∏çÊîØÊåÅ inplace Êìç‰Ωú„ÄÇ\n",
    "# ÊúÄÂ•ΩÂú®ÁªßÁª≠Â§ÑÁêÜÂâçÂ∞ùËØïÂº∫Âà∂ËΩ¨Êç¢‰∏∫ÂÜÖÂ≠òÂØπË±°ÔºàÂ¶ÇÊûúÊúâÂÜÖÂ≠òÂâ©‰ΩôÔºâ„ÄÇ\n",
    "if adata.is_view:\n",
    "    adata = adata.copy() # ÊàñËÄÖ .to_memory()\n",
    "\n",
    "# ÂΩí‰∏ÄÂåñÊìç‰ΩúÈÄöÂ∏∏‰∏ç‰ºöÊúâ MemoryError\n",
    "sc.pp.normalize_total(adata, target_sum=1e4)\n",
    "print(\"‚úÖ ÂΩí‰∏ÄÂåñÂÆåÊàê„ÄÇ\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36d6aff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nodes: torch.Size([144249, 35])\n",
      "Domains: 43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouyan\\AppData\\Local\\Temp\\ipykernel_11396\\3071671118.py:7: FutureWarning: Use obsm (e.g. `k in adata.obsm` or `adata.obsm.keys() | {'u'}`) instead of AnnData.obsm_keys, AnnData.obsm_keys is deprecated and will be removed in the future.\n",
      "  if 'X_spatial_coords' in adata.obsm_keys():\n"
     ]
    }
   ],
   "source": [
    "# 1. ÊääÊØè‰∏™ cell ÁöÑ PCA ÁâπÂæÅËΩ¨Êç¢Êàê PyTorch tensorÔºåÁî®‰Ωú ËäÇÁÇπÁâπÂæÅ (node features)\n",
    "X_pca = adata.obsm['X_pca']             # (N_cells, 50 PCs)\n",
    "X = torch.tensor(X_pca, dtype=torch.float)\n",
    "\n",
    "# 2. Ëé∑ÂèñÊØè‰∏™ÁªÜËÉûÁöÑÁ©∫Èó¥ÂùêÊ†áÁî®Êù•ÊûÑÂª∫ Á©∫Èó¥ kNN Âõæ\n",
    "# ========== Spatial coordinates:‰ºòÂÖà‰ΩøÁî®X_spatial_coords ==========\n",
    "if 'X_spatial_coords' in adata.obsm_keys():\n",
    "    coords = adata.obsm['X_spatial_coords']\n",
    "elif 'spatial_coords' in adata.obsm_keys():\n",
    "    coords = adata.obsm['spatial_coords']\n",
    "else:\n",
    "    coords = adata.obs[['center_x', 'center_y']].to_numpy()\n",
    "\n",
    "coords = coords.astype(float)\n",
    "n_cells = coords.shape[0]\n",
    "# 3. Ëé∑ÂèñÊØè‰∏™ÁªÜËÉûÊâÄÂ±ûÁöÑ domainÔºàÂå∫Âüü„ÄÅclusterÔºâÔºåÊûÑÂª∫ domain-domain graph (DDG)\n",
    "# ========== Domain labels ==========\n",
    "domains = adata.obs['clust_annot'].astype('category')\n",
    "domain_idx = domains.cat.codes.to_numpy()      # integer labels\n",
    "n_domains = len(domains.cat.categories)\n",
    "\n",
    "print(\"Nodes:\", X.shape)\n",
    "print(\"Domains:\", n_domains)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3823b77a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spatial edges: 1317660\n"
     ]
    }
   ],
   "source": [
    "# ----- Build spatial kNN graph -----\n",
    "k = 8\n",
    "n_cells = coords.shape[0]\n",
    "\n",
    "nbrs = NearestNeighbors(n_neighbors=k+1, algorithm='ball_tree')\n",
    "nbrs.fit(coords)\n",
    "distances, indices = nbrs.kneighbors(coords)\n",
    "\n",
    "rows = []\n",
    "cols = []\n",
    "weights = []\n",
    "\n",
    "for i in range(n_cells):\n",
    "    for j, d in zip(indices[i, 1:], distances[i, 1:]):\n",
    "        rows.append(i)\n",
    "        cols.append(j)\n",
    "        weights.append(np.exp(-d))\n",
    "\n",
    "# Build sparse adjacency in COO format\n",
    "A_spatial = sp.coo_matrix((weights, (rows, cols)), shape=(coords.shape[0], coords.shape[0]))\n",
    "# Symmetrize adjacency\n",
    "A_spatial = A_spatial.maximum(A_spatial.T)  # ÂØπÁß∞Âåñ\n",
    "\n",
    "# üö® ‰øÆÊ≠£ÔºöÂ∞ÜÁü©ÈòµËΩ¨Êç¢Âõû COO Ê†ºÂºè‰ª•Ëé∑Âèñ row Âíå col Â±ûÊÄß\n",
    "if not isinstance(A_spatial, sp.coo_matrix):\n",
    "    A_spatial = A_spatial.tocoo()\n",
    "    \n",
    "# Now you can safely access row, col\n",
    "edge_index = torch.tensor(np.vstack([A_spatial.row, A_spatial.col]), dtype=torch.long)\n",
    "edge_attr  = torch.tensor(A_spatial.data, dtype=torch.float)\n",
    "print(\"Spatial edges:\", edge_index.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1b735ac4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Ê£ÄÊü•ÈÄöËøáÔºöDD_counts Áü©ÈòµÊòØÂØπÁß∞ÁöÑ (ÂΩ¢Áä∂: (43, 43))„ÄÇ\n"
     ]
    }
   ],
   "source": [
    "# Map domain names to numeric indices\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains.cat.categories)}\n",
    "domain_per_cell = domain_idx\n",
    "\n",
    "# Initialize domain adjacency counts\n",
    "DD_counts = np.zeros((n_domains, n_domains), dtype=float)\n",
    "\n",
    "# ÂÅáËÆæ DD_counts ÊòØÊÇ®ËÆ°ÁÆóÂæóÂà∞ÁöÑÂüü-ÂüüËøûÊé•ËÆ°Êï∞Áü©Èòµ\n",
    "\n",
    "# ----------------------------------------------------\n",
    "# üìå ÂØπÁß∞ÊÄßÊ£ÄÊü•‰ª£Á†Å\n",
    "# ----------------------------------------------------\n",
    "\n",
    "# 1. Ê£ÄÊü• DD_counts ÁöÑÁª¥Â∫¶\n",
    "if DD_counts.ndim != 2 or DD_counts.shape[0] != DD_counts.shape[1]:\n",
    "    print(\"‚ùå Ê£ÄÊü•Â§±Ë¥•ÔºöDD_counts ‰∏çÊòØ‰∏Ä‰∏™ÊñπÈòµÔºåÊó†Ê≥ïÊ£ÄÊü•ÂØπÁß∞ÊÄß„ÄÇ\")\n",
    "else:\n",
    "    # ËÆ°ÁÆóËΩ¨ÁΩÆÁü©Èòµ\n",
    "    DD_counts_T = DD_counts.T\n",
    "    \n",
    "    # Ê£ÄÊü•ÊâÄÊúâÂÖÉÁ¥†ÊòØÂê¶Áõ∏Á≠â\n",
    "    # np.allclose ÈÄÇÁî®‰∫éÊµÆÁÇπÊï∞ÊØîËæÉÔºåÂç≥‰Ωø DD_counts ÊòØÊï¥Êï∞Ôºå‰ΩøÁî®ÂÆÉ‰πüÊõ¥ÂÆâÂÖ®\n",
    "    is_symmetric = np.allclose(DD_counts, DD_counts_T)\n",
    "    \n",
    "    if is_symmetric:\n",
    "        print(f\"‚úÖ Ê£ÄÊü•ÈÄöËøáÔºöDD_counts Áü©ÈòµÊòØÂØπÁß∞ÁöÑ (ÂΩ¢Áä∂: {DD_counts.shape})„ÄÇ\")\n",
    "    else:\n",
    "        print(f\"‚ùå Ê£ÄÊü•Â§±Ë¥•ÔºöDD_counts Áü©Èòµ‰∏çÊòØÂØπÁß∞ÁöÑ (ÂΩ¢Áä∂: {DD_counts.shape})„ÄÇ\")\n",
    "        \n",
    "        # ÂèØÈÄâÔºöÊ£ÄÊü•Â∑ÆÂºÇÊúÄÂ§ßÁöÑ‰ΩçÁΩÆÂíåÊï∞ÂÄº\n",
    "        # ËÆ°ÁÆóÂ∑ÆÂºÇÁü©ÈòµÁöÑÁªùÂØπÂÄº\n",
    "        diff_matrix = np.abs(DD_counts - DD_counts_T)\n",
    "        max_diff = np.max(diff_matrix)\n",
    "        \n",
    "        if max_diff > 0:\n",
    "            # ÊâæÂà∞Â∑ÆÂºÇÊúÄÂ§ßÁöÑÂÖÉÁ¥†ÁöÑÁ¥¢Âºï\n",
    "            # Ê≥®ÊÑèÔºönp.argmax ËøîÂõûÊâÅÂπ≥Êï∞ÁªÑÁöÑÁ¥¢ÂºïÔºåÈúÄË¶ÅÁî® np.unravel_index ËΩ¨Êç¢\n",
    "            max_index_flat = np.argmax(diff_matrix)\n",
    "            max_index = np.unravel_index(max_index_flat, DD_counts.shape)\n",
    "            \n",
    "            print(f\"   ÊúÄÂ§ß‰∏çÂØπÁß∞Â∑ÆÂºÇ: {max_diff:.4f}\")\n",
    "            print(f\"   ÂèëÁîüÂú®Á¥¢Âºï {max_index}: DD_counts[{max_index[0]}, {max_index[1]}] = {DD_counts[max_index]:.0f}, DD_counts_T[{max_index[0]}, {max_index[1]}] = {DD_counts_T[max_index]:.0f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d372beb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DDG edges: 503\n",
      "Data(x=[144249, 35], edge_index=[2, 1317660], edge_attr=[1317660], y_domain=[144249], n_domains=43, DD_edge_index=[2, 503], DD_edge_attr=[503])\n"
     ]
    }
   ],
   "source": [
    "# Map domain names to numeric indices\n",
    "domain_to_idx = {d: i for i, d in enumerate(domains.cat.categories)}\n",
    "domain_per_cell = domain_idx\n",
    "\n",
    "# Initialize domain adjacency counts\n",
    "DD_counts = np.zeros((n_domains, n_domains), dtype=float)\n",
    "\n",
    "rows = A_spatial.row\n",
    "cols = A_spatial.col\n",
    "\n",
    "for i_cell, j_cell in zip(rows, cols):\n",
    "    d_i = domain_per_cell[i_cell]\n",
    "    d_j = domain_per_cell[j_cell]\n",
    "    DD_counts[d_i, d_j] += 1\n",
    "    DD_counts[d_j, d_i] += 1\n",
    "\n",
    "eps = 1e-8\n",
    "DD_phys = DD_counts / (DD_counts.sum(axis=1, keepdims=True) + eps)\n",
    "\n",
    "# Compute domain mean vectors in PCA space\n",
    "domain_mean = np.zeros((n_domains, X_pca.shape[1]))\n",
    "domain_counts = np.zeros(n_domains)\n",
    "\n",
    "for cell, d in enumerate(domain_per_cell):\n",
    "    domain_mean[d] += X_pca[cell]\n",
    "    domain_counts[d] += 1\n",
    "\n",
    "domain_counts[domain_counts == 0] = 1\n",
    "domain_mean = domain_mean / domain_counts[:, None]\n",
    "\n",
    "from numpy.linalg import norm\n",
    "\n",
    "DD_sem = np.zeros((n_domains, n_domains))\n",
    "\n",
    "for i in range(n_domains):\n",
    "    for j in range(n_domains):\n",
    "        vi, vj = domain_mean[i], domain_mean[j]\n",
    "        DD_sem[i, j] = np.dot(vi, vj) / (norm(vi)*norm(vj) + eps)\n",
    "\n",
    "alpha = 0.5   # weight between physical and semantic\n",
    "DD_combined = alpha * DD_phys + (1 - alpha) * DD_sem\n",
    "\n",
    "# threshold weak edges\n",
    "thr = 0.1\n",
    "DD_mask = (DD_combined > thr).astype(float)\n",
    "DD_final = DD_combined * DD_mask\n",
    "\n",
    "DD_rows, DD_cols = np.where(DD_final > 0)\n",
    "DD_weights = DD_final[DD_rows, DD_cols]\n",
    "\n",
    "DD_edge_index = torch.tensor(np.vstack([DD_rows, DD_cols]), dtype=torch.long)\n",
    "DD_edge_attr  = torch.tensor(DD_weights, dtype=torch.float)\n",
    "\n",
    "print(\"DDG edges:\", DD_edge_index.shape[1])\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "data = Data(\n",
    "    x=X,                        # PCA features\n",
    "    edge_index=edge_index,      # spatial graph edges\n",
    "    edge_attr=edge_attr         # spatial edge weights\n",
    ")\n",
    "\n",
    "# Add domain info + DDG\n",
    "data.y_domain = torch.tensor(domain_per_cell, dtype=torch.long)\n",
    "data.n_domains = n_domains\n",
    "data.DD_edge_index = DD_edge_index\n",
    "data.DD_edge_attr  = DD_edge_attr\n",
    "\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "9fde07d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GraphConv  # you can switch to GCNConv or GATConv\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "# ============================\n",
    "# 1. Encoder: MLP + GNN ‚Üí Œº, logœÉ\n",
    "# ============================\n",
    "\n",
    "class DRASIEncoder(nn.Module):\n",
    "    def __init__(self, in_dim, hidden_mlp=128, hidden_gnn=128, latent_dim=32):\n",
    "        super().__init__()\n",
    "        # MLP to compress input features (PCA) before GNN\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(in_dim, hidden_mlp),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_mlp, hidden_gnn),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        # GNN layers over spatial graph\n",
    "        self.gnn1 = GraphConv(hidden_gnn, hidden_gnn)\n",
    "        self.gnn2 = GraphConv(hidden_gnn, hidden_gnn)\n",
    "\n",
    "        # Heads for VAE parameters\n",
    "        self.mu_head     = nn.Linear(hidden_gnn, latent_dim)\n",
    "        self.logvar_head = nn.Linear(hidden_gnn, latent_dim)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_attr):\n",
    "        # x: (N, in_dim), edge_index: (2, E)\n",
    "        h = self.mlp(x)\n",
    "        # Ê†∏ÂøÉ‰øÆÊ≠£ÔºöÂ∞Ü edge_attr ‰º†ÈÄíÁªô GraphConv\n",
    "        h = F.relu(self.gnn1(h, edge_index, edge_attr))\n",
    "        h = F.relu(self.gnn2(h, edge_index, edge_attr))\n",
    "        mu     = self.mu_head(h)\n",
    "        logvar = self.logvar_head(h)\n",
    "        return mu, logvar\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 2. Reparameterization\n",
    "# =======================\n",
    "\n",
    "def reparameterize(mu, logvar):\n",
    "    std = torch.exp(0.5 * logvar)\n",
    "    eps = torch.randn_like(std)\n",
    "    return mu + eps * std\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 3. Decoder: z ‚Üí xÃÇ\n",
    "# =======================\n",
    "\n",
    "class DRASIDecoder(nn.Module):\n",
    "    def __init__(self, latent_dim, hidden_dec=128, out_dim=None):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dec),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dec, out_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, z):\n",
    "        return self.net(z)\n",
    "\n",
    "\n",
    "# =======================\n",
    "# 4. Full DRA-SI Model\n",
    "# =======================\n",
    "\n",
    "class DRASIModel(nn.Module):\n",
    "    def __init__(self, in_dim, n_domains, latent_dim=32):\n",
    "        super().__init__()\n",
    "        self.encoder = DRASIEncoder(in_dim, latent_dim=latent_dim)\n",
    "        self.decoder = DRASIDecoder(latent_dim, out_dim=in_dim)\n",
    "        self.n_domains = n_domains\n",
    "\n",
    "    def forward(self, data):\n",
    "        # data.x, data.edge_index\n",
    "        x = data.x\n",
    "        edge_index = data.edge_index\n",
    "        edge_attr = data.edge_attr\n",
    "\n",
    "        mu, logvar = self.encoder(x, edge_index, edge_attr)\n",
    "        z = reparameterize(mu, logvar)\n",
    "        x_recon = self.decoder(z)\n",
    "\n",
    "        return x_recon, mu, logvar, z\n",
    "\n",
    "\n",
    "# ======================================\n",
    "# 5. Loss components: Recon + KL + DDG\n",
    "# ======================================\n",
    "\n",
    "def loss_recon(x, x_recon):\n",
    "    # MSE between reconstructed and original PCA features\n",
    "    return F.mse_loss(x_recon, x)\n",
    "\n",
    "def loss_kl(mu, logvar):\n",
    "    # KL divergence between q(z|x) and N(0, I)\n",
    "    return -0.5 * torch.mean(1 + logvar - mu.pow(2) - logvar.exp())\n",
    "\n",
    "def compute_domain_means(z, domain_idx, n_domains):\n",
    "    \"\"\"\n",
    "    z: (N, latent_dim)\n",
    "    domain_idx: (N,) int\n",
    "    Returns: (n_domains, latent_dim) mean embeddings.\n",
    "    \"\"\"\n",
    "    latent_dim = z.size(1)\n",
    "    device = z.device\n",
    "\n",
    "    sums = torch.zeros(n_domains, latent_dim, device=device)\n",
    "    counts = torch.zeros(n_domains, device=device)\n",
    "\n",
    "    # accumulate sums & counts\n",
    "    sums.index_add_(0, domain_idx, z)\n",
    "    ones = torch.ones_like(domain_idx, dtype=torch.float, device=device)\n",
    "    counts.index_add_(0, domain_idx, ones)\n",
    "\n",
    "    counts = counts.clamp(min=1.0)\n",
    "    means = sums / counts.unsqueeze(1)\n",
    "    return means\n",
    "\n",
    "def loss_ddg(z, domain_idx, DD_edge_index, DD_edge_attr, n_domains, lambda_ddg=1.0):\n",
    "    \"\"\"\n",
    "    z: (N, latent_dim)\n",
    "    domain_idx: (N,) int\n",
    "    DD_edge_index: (2, E_D)\n",
    "    DD_edge_attr: (E_D,)\n",
    "    \"\"\"\n",
    "    device = z.device\n",
    "    DD_edge_index = DD_edge_index.to(device)\n",
    "    DD_edge_attr  = DD_edge_attr.to(device)\n",
    "    domain_idx    = domain_idx.to(device)\n",
    "\n",
    "    domain_means = compute_domain_means(z, domain_idx, n_domains)  # (D, latent_dim)\n",
    "\n",
    "    d_i = DD_edge_index[0]  # (E_D,)\n",
    "    d_j = DD_edge_index[1]\n",
    "\n",
    "    mu_i = domain_means[d_i]   # (E_D, latent_dim)\n",
    "    mu_j = domain_means[d_j]\n",
    "\n",
    "    diff = mu_i - mu_j\n",
    "    dist_sq = (diff * diff).sum(dim=1)   # (E_D,)\n",
    "\n",
    "    weighted = DD_edge_attr * dist_sq\n",
    "    return lambda_ddg * weighted.mean()\n",
    "\n",
    "\n",
    "# ======================\n",
    "# 6. Total loss wrapper\n",
    "# ======================\n",
    "\n",
    "def total_loss(data, model, lambda_kl=1e-3, lambda_ddg=0.1):\n",
    "    \"\"\"\n",
    "    data: PyG Data\n",
    "    model: DRASIModel\n",
    "    \"\"\"\n",
    "    x = data.x\n",
    "    y_domain = data.y_domain\n",
    "    DD_edge_index = data.DD_edge_index\n",
    "    DD_edge_attr  = data.DD_edge_attr\n",
    "    n_domains = data.n_domains\n",
    "\n",
    "    x_recon, mu, logvar, z = model(data)\n",
    "\n",
    "    L_rec = loss_recon(x, x_recon)\n",
    "    L_kl  = loss_kl(mu, logvar)\n",
    "    L_DDG = loss_ddg(z, y_domain, DD_edge_index, DD_edge_attr, n_domains, lambda_ddg=lambda_ddg)\n",
    "\n",
    "    L = L_rec + lambda_kl * L_kl + L_DDG\n",
    "    logs = {\n",
    "        \"loss\": L.item(),\n",
    "        \"rec\": L_rec.item(),\n",
    "        \"kl\": L_kl.item(),\n",
    "        \"ddg\": L_DDG.item()\n",
    "    }\n",
    "    return L, logs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f415d36d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | loss=0.0890 | rec=0.0868 | kl=0.0024 | ddg=0.0022\n",
      "Epoch 010 | loss=0.0523 | rec=0.0505 | kl=0.0110 | ddg=0.0017\n",
      "Epoch 020 | loss=0.0356 | rec=0.0342 | kl=0.3324 | ddg=0.0011\n",
      "Epoch 030 | loss=0.0308 | rec=0.0292 | kl=1.3546 | ddg=0.0002\n",
      "Epoch 040 | loss=0.0298 | rec=0.0283 | kl=1.3570 | ddg=0.0001\n",
      "Epoch 050 | loss=0.0288 | rec=0.0273 | kl=1.2542 | ddg=0.0002\n",
      "Epoch 060 | loss=0.0268 | rec=0.0250 | kl=1.2913 | ddg=0.0005\n",
      "Epoch 070 | loss=0.0243 | rec=0.0220 | kl=1.4059 | ddg=0.0009\n",
      "Epoch 080 | loss=0.0230 | rec=0.0209 | kl=1.4570 | ddg=0.0006\n",
      "Epoch 090 | loss=0.0224 | rec=0.0203 | kl=1.4873 | ddg=0.0007\n",
      "Epoch 100 | loss=0.0217 | rec=0.0192 | kl=1.5658 | ddg=0.0009\n",
      "Latent embedding shape: (144249, 32)\n",
      "‚úÖ Latent embeddings successfully stored in adata.obsm['X_drasi'].\n",
      "New obsm keys: ['X_pca', 'X_spatial_coords', 'X_umap', 'spatial_coords', 'X_drasi']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ouyan\\AppData\\Local\\Temp\\ipykernel_11396\\203807910.py:43: FutureWarning: Use obsm (e.g. `k in adata.obsm` or `adata.obsm.keys() | {'u'}`) instead of AnnData.obsm_keys, AnnData.obsm_keys is deprecated and will be removed in the future.\n",
      "  print(\"New obsm keys:\", adata.obsm_keys())\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "in_dim = data.x.size(1)\n",
    "n_domains = int(data.n_domains)\n",
    "\n",
    "model = DRASIModel(in_dim=in_dim, n_domains=n_domains, latent_dim=32).to(device)\n",
    "data = data.to(device)\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "n_epochs = 100  # you can increase to 200‚Äì300 once things work\n",
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    L, logs = total_loss(data, model, lambda_kl=1e-3, lambda_ddg=0.1)\n",
    "    L.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 10 == 0 or epoch == 1:\n",
    "        print(\n",
    "            f\"Epoch {epoch:03d} | \"\n",
    "            f\"loss={logs['loss']:.4f} | \"\n",
    "            f\"rec={logs['rec']:.4f} | \"\n",
    "            f\"kl={logs['kl']:.4f} | \"\n",
    "            f\"ddg={logs['ddg']:.4f}\"\n",
    "        )\n",
    "\n",
    "# After training, get latent embeddings\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    _, _, _, z = model(data)   # z: (N_cells, latent_dim)\n",
    "\n",
    "z_cpu = z.cpu().numpy()\n",
    "print(\"Latent embedding shape:\", z_cpu.shape)\n",
    "\n",
    "# ÂÖ≥ÈîÆÊ≠•È™§ÔºöÂ∞ÜÊΩúÂú®Ë°®Á§∫Â≠òÂõû AnnData\n",
    "# Z ÁöÑË°åÊï∞ (N_cells) ÂøÖÈ°ªÂíå adata ÁöÑ n_obs ÂåπÈÖç\n",
    "adata.obsm['X_drasi'] = z_cpu\n",
    "\n",
    "print(\"‚úÖ Latent embeddings successfully stored in adata.obsm['X_drasi'].\")\n",
    "print(\"New obsm keys:\", adata.obsm_keys())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
